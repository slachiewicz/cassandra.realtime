Spark Executor Command: "/home/gitpod/.sdkman/candidates/java/current/bin/java" "-cp" "/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/conf/:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=42403" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403" "--executor-id" "0" "--hostname" "10.8.9.203" "--cores" "5" "--app-id" "app-20201117213023-0018" "--worker-url" "spark://Worker@10.8.9.203:37659"
========================================

Picked up JAVA_TOOL_OPTIONS: -Xmx2254m
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/11/17 21:30:25 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 106745@ws-763ae24b-f444-4ec9-ab04-58a9b208cf41
20/11/17 21:30:25 INFO SignalUtils: Registered signal handler for TERM
20/11/17 21:30:25 INFO SignalUtils: Registered signal handler for HUP
20/11/17 21:30:25 INFO SignalUtils: Registered signal handler for INT
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
20/11/17 21:30:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/17 21:30:26 INFO SecurityManager: Changing view acls to: gitpod
20/11/17 21:30:26 INFO SecurityManager: Changing modify acls to: gitpod
20/11/17 21:30:26 INFO SecurityManager: Changing view acls groups to: 
20/11/17 21:30:26 INFO SecurityManager: Changing modify acls groups to: 
20/11/17 21:30:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gitpod); groups with view permissions: Set(); users  with modify permissions: Set(gitpod); groups with modify permissions: Set()
20/11/17 21:30:26 INFO TransportClientFactory: Successfully created connection to ws-763ae24b-f444-4ec9-ab04-58a9b208cf41/10.8.9.203:42403 after 93 ms (0 ms spent in bootstraps)
20/11/17 21:30:26 INFO SecurityManager: Changing view acls to: gitpod
20/11/17 21:30:26 INFO SecurityManager: Changing modify acls to: gitpod
20/11/17 21:30:26 INFO SecurityManager: Changing view acls groups to: 
20/11/17 21:30:26 INFO SecurityManager: Changing modify acls groups to: 
20/11/17 21:30:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gitpod); groups with view permissions: Set(); users  with modify permissions: Set(gitpod); groups with modify permissions: Set()
20/11/17 21:30:27 INFO TransportClientFactory: Successfully created connection to ws-763ae24b-f444-4ec9-ab04-58a9b208cf41/10.8.9.203:42403 after 4 ms (0 ms spent in bootstraps)
20/11/17 21:30:27 INFO DiskBlockManager: Created local directory at /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/blockmgr-bed935bf-2884-4e0d-a09d-d6a1379cf137
20/11/17 21:30:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
20/11/17 21:30:27 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403
20/11/17 21:30:27 INFO WorkerWatcher: Connecting to worker spark://Worker@10.8.9.203:37659
20/11/17 21:30:27 INFO TransportClientFactory: Successfully created connection to /10.8.9.203:37659 after 5 ms (0 ms spent in bootstraps)
20/11/17 21:30:27 INFO WorkerWatcher: Successfully connected to spark://Worker@10.8.9.203:37659
20/11/17 21:30:27 INFO ResourceUtils: ==============================================================
20/11/17 21:30:27 INFO ResourceUtils: Resources for spark.executor:

20/11/17 21:30:27 INFO ResourceUtils: ==============================================================
20/11/17 21:30:27 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
20/11/17 21:30:27 INFO Executor: Starting executor ID 0 on host 10.8.9.203
20/11/17 21:30:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41531.
20/11/17 21:30:27 INFO NettyBlockTransferService: Server created on 10.8.9.203:41531
20/11/17 21:30:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/17 21:30:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 10.8.9.203, 41531, None)
20/11/17 21:30:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 10.8.9.203, 41531, None)
20/11/17 21:30:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 10.8.9.203, 41531, None)
20/11/17 21:30:30 INFO CoarseGrainedExecutorBackend: Got assigned task 0
20/11/17 21:30:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/files/secure-connect-demo.zip with timestamp 1605648623065
20/11/17 21:30:30 INFO TransportClientFactory: Successfully created connection to ws-763ae24b-f444-4ec9-ab04-58a9b208cf41/10.8.9.203:42403 after 3 ms (0 ms spent in bootstraps)
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/files/secure-connect-demo.zip to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp7110085944469164470.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/8711850981605648623065_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./secure-connect-demo.zip
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.kafka_kafka-clients-2.4.1.jar with timestamp 1605648623062
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.kafka_kafka-clients-2.4.1.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp2361817648647449069.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/-21104369371605648623062_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.kafka_kafka-clients-2.4.1.jar
20/11/17 21:30:30 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.kafka_kafka-clients-2.4.1.jar to class loader
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1605648623063
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp6872868930767531156.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/3902329891605648623063_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.xerial.snappy_snappy-java-1.1.7.3.jar
20/11/17 21:30:30 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.xerial.snappy_snappy-java-1.1.7.3.jar to class loader
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1605648623062
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp13493892089543130542.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/-14221392521605648623062_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.commons_commons-pool2-2.6.2.jar
20/11/17 21:30:30 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.commons_commons-pool2-2.6.2.jar to class loader
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.1.jar with timestamp 1605648623061
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.1.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp27259552447392590.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/6344708681605648623061_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.1.jar
20/11/17 21:30:30 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.1.jar to class loader
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.1.jar with timestamp 1605648623062
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.1.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp1371875508327352809.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/8787249351605648623062_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.1.jar
20/11/17 21:30:30 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.1.jar to class loader
20/11/17 21:30:30 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1605648623062
20/11/17 21:30:30 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp10748707282568435822.tmp
20/11/17 21:30:30 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/9241453181605648623062_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.spark-project.spark_unused-1.0.0.jar
20/11/17 21:30:31 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.spark-project.spark_unused-1.0.0.jar to class loader
20/11/17 21:30:31 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.slf4j_slf4j-api-1.7.28.jar with timestamp 1605648623063
20/11/17 21:30:31 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.slf4j_slf4j-api-1.7.28.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp17938910663705281007.tmp
20/11/17 21:30:31 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/20865439241605648623063_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.slf4j_slf4j-api-1.7.28.jar
20/11/17 21:30:31 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.slf4j_slf4j-api-1.7.28.jar to class loader
20/11/17 21:30:31 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.lz4_lz4-java-1.6.0.jar with timestamp 1605648623063
20/11/17 21:30:31 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/org.lz4_lz4-java-1.6.0.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp16380981489180052769.tmp
20/11/17 21:30:31 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/-19444667671605648623063_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.lz4_lz4-java-1.6.0.jar
20/11/17 21:30:31 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./org.lz4_lz4-java-1.6.0.jar to class loader
20/11/17 21:30:31 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/spark-cassandra_2.12-0.1.0-SNAPSHOT.jar with timestamp 1605648623063
20/11/17 21:30:31 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/spark-cassandra_2.12-0.1.0-SNAPSHOT.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp5037717941404585111.tmp
20/11/17 21:30:31 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/17506392081605648623063_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./spark-cassandra_2.12-0.1.0-SNAPSHOT.jar
20/11/17 21:30:31 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./spark-cassandra_2.12-0.1.0-SNAPSHOT.jar to class loader
20/11/17 21:30:31 INFO Executor: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/com.github.luben_zstd-jni-1.4.3-1.jar with timestamp 1605648623062
20/11/17 21:30:31 INFO Utils: Fetching spark://ws-763ae24b-f444-4ec9-ab04-58a9b208cf41:42403/jars/com.github.luben_zstd-jni-1.4.3-1.jar to /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/fetchFileTemp10414675089223683991.tmp
20/11/17 21:30:31 INFO Utils: Copying /tmp/spark-7d48140b-9f01-48ee-a4e9-251e0afff9c6/executor-aa981e6d-20af-4609-812a-64db28c6a96c/spark-dbdd9074-f8f2-44ce-a9d0-d28096ae93c2/-2254357581605648623062_cache to /workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./com.github.luben_zstd-jni-1.4.3-1.jar
20/11/17 21:30:31 INFO Executor: Adding file:/workspace/cassandra.realtime/spark/spark-3.0.1-bin-hadoop2.7/work/app-20201117213023-0018/0/./com.github.luben_zstd-jni-1.4.3-1.jar to class loader
20/11/17 21:30:31 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:31 INFO TransportClientFactory: Successfully created connection to ws-763ae24b-f444-4ec9-ab04-58a9b208cf41/10.8.9.203:40109 after 2 ms (0 ms spent in bootstraps)
20/11/17 21:30:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1485.0 B, free 434.4 MiB)
20/11/17 21:30:31 INFO TorrentBroadcast: Reading broadcast variable 0 took 160 ms
20/11/17 21:30:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.3 KiB, free 434.4 MiB)
20/11/17 21:30:31 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
20/11/17 21:30:31 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
20/11/17 21:30:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1240 bytes result sent to driver
20/11/17 21:30:45 INFO CoarseGrainedExecutorBackend: Got assigned task 1
20/11/17 21:30:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/11/17 21:30:45 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:45 INFO TorrentBroadcast: Reading broadcast variable 1 took 16 ms
20/11/17 21:30:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:46 INFO CodeGenerator: Code generated in 278.799644 ms
20/11/17 21:30:46 INFO CodeGenerator: Code generated in 46.601627 ms
20/11/17 21:30:46 INFO CodeGenerator: Code generated in 28.259577 ms
20/11/17 21:30:46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

20/11/17 21:30:47 INFO AppInfoParser: Kafka version: 2.4.1
20/11/17 21:30:47 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
20/11/17 21:30:47 INFO AppInfoParser: Kafka startTimeMs: 1605648647012
20/11/17 21:30:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Subscribed to partition(s): record-cassandra-leaves-avro-0
20/11/17 21:30:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 19980 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:47 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Cluster ID: NRzzcZosTEOCy6S9R5oouw
20/11/17 21:30:48 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
20/11/17 21:30:48 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
20/11/17 21:30:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 111648 bytes result sent to driver
20/11/17 21:30:48 INFO CoarseGrainedExecutorBackend: Got assigned task 2
20/11/17 21:30:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
20/11/17 21:30:48 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:48 INFO TorrentBroadcast: Reading broadcast variable 2 took 18 ms
20/11/17 21:30:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20002 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20030 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20052 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20072 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20101 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20124 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20143 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20169 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20197 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20224 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20250 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20270 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20301 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:48 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
20/11/17 21:30:48 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
20/11/17 21:30:48 INFO MemoryStore: Block taskresult_2 stored as bytes in memory (estimated size 12.9 MiB, free 421.5 MiB)
20/11/17 21:30:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 13566950 bytes result sent via BlockManager)
20/11/17 21:30:49 INFO CoarseGrainedExecutorBackend: Got assigned task 3
20/11/17 21:30:49 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
20/11/17 21:30:49 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:49 INFO TorrentBroadcast: Reading broadcast variable 3 took 15 ms
20/11/17 21:30:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20331 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20368 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20421 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:49 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
20/11/17 21:30:49 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
20/11/17 21:30:49 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.7 MiB, free 431.7 MiB)
20/11/17 21:30:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2819562 bytes result sent via BlockManager)
20/11/17 21:30:49 INFO CoarseGrainedExecutorBackend: Got assigned task 4
20/11/17 21:30:49 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
20/11/17 21:30:49 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:49 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:49 INFO TorrentBroadcast: Reading broadcast variable 4 took 12 ms
20/11/17 21:30:49 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20467 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20469 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:49 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
20/11/17 21:30:49 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
20/11/17 21:30:49 INFO MemoryStore: Block taskresult_4 stored as bytes in memory (estimated size 1097.6 KiB, free 433.3 MiB)
20/11/17 21:30:49 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1123973 bytes result sent via BlockManager)
20/11/17 21:30:50 INFO CoarseGrainedExecutorBackend: Got assigned task 5
20/11/17 21:30:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
20/11/17 21:30:50 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO TorrentBroadcast: Reading broadcast variable 5 took 13 ms
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20497 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20506 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
20/11/17 21:30:50 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
20/11/17 21:30:50 INFO MemoryStore: Block taskresult_5 stored as bytes in memory (estimated size 1770.5 KiB, free 432.7 MiB)
20/11/17 21:30:50 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1812995 bytes result sent via BlockManager)
20/11/17 21:30:50 INFO CoarseGrainedExecutorBackend: Got assigned task 6
20/11/17 21:30:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
20/11/17 21:30:50 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO TorrentBroadcast: Reading broadcast variable 6 took 9 ms
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20530 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20540 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20547 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20548 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
20/11/17 21:30:50 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
20/11/17 21:30:50 INFO MemoryStore: Block taskresult_6 stored as bytes in memory (estimated size 2.4 MiB, free 432.0 MiB)
20/11/17 21:30:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2529699 bytes result sent via BlockManager)
20/11/17 21:30:50 INFO CoarseGrainedExecutorBackend: Got assigned task 7
20/11/17 21:30:50 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
20/11/17 21:30:50 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO TorrentBroadcast: Reading broadcast variable 7 took 9 ms
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20573 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20574 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
20/11/17 21:30:50 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
20/11/17 21:30:50 INFO MemoryStore: Block taskresult_7 stored as bytes in memory (estimated size 1435.5 KiB, free 433.0 MiB)
20/11/17 21:30:50 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1469955 bytes result sent via BlockManager)
20/11/17 21:30:50 INFO CoarseGrainedExecutorBackend: Got assigned task 8
20/11/17 21:30:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
20/11/17 21:30:50 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO TorrentBroadcast: Reading broadcast variable 8 took 11 ms
20/11/17 21:30:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20598 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20609 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:50 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
20/11/17 21:30:50 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
20/11/17 21:30:50 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1019214 bytes result sent to driver
20/11/17 21:30:51 INFO CoarseGrainedExecutorBackend: Got assigned task 9
20/11/17 21:30:51 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
20/11/17 21:30:51 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:51 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:51 INFO TorrentBroadcast: Reading broadcast variable 9 took 8 ms
20/11/17 21:30:51 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20630 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20637 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:51 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
20/11/17 21:30:51 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
20/11/17 21:30:51 INFO MemoryStore: Block taskresult_9 stored as bytes in memory (estimated size 1117.8 KiB, free 433.3 MiB)
20/11/17 21:30:51 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1144626 bytes result sent via BlockManager)
20/11/17 21:30:51 INFO CoarseGrainedExecutorBackend: Got assigned task 10
20/11/17 21:30:51 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
20/11/17 21:30:51 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:51 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:51 INFO TorrentBroadcast: Reading broadcast variable 10 took 9 ms
20/11/17 21:30:51 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20665 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20666 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:51 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
20/11/17 21:30:51 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
20/11/17 21:30:51 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 529566 bytes result sent to driver
20/11/17 21:30:51 INFO CoarseGrainedExecutorBackend: Got assigned task 11
20/11/17 21:30:51 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
20/11/17 21:30:51 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
20/11/17 21:30:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.4 MiB)
20/11/17 21:30:51 INFO TorrentBroadcast: Reading broadcast variable 11 took 10 ms
20/11/17 21:30:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.2 KiB, free 434.4 MiB)
20/11/17 21:30:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20690 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor-1, groupId=spark-kafka-source-f6236962-723e-4367-b3b3-4134bafa8806-58497883-executor] Seeking to offset 20691 for partition record-cassandra-leaves-avro-0
20/11/17 21:30:51 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 11, attempt 0, stage 11.0)
20/11/17 21:30:51 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
20/11/17 21:30:51 INFO MemoryStore: Block taskresult_11 stored as bytes in memory (estimated size 1148.9 KiB, free 433.2 MiB)
20/11/17 21:30:51 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1176450 bytes result sent via BlockManager)
20/11/17 21:30:51 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/11/17 21:30:51 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
